\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}

\aclfinalcopy % Print article as final version
\setlength\titlebox{10cm} % Create vertical space for the authors
\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Complex word identification and replacement for a more accessible web experience in Mashable.com}

\author{Gerardo Quiros Sandoval \\
  Universidad Carlos III de Madrid \\
  Avda. Universidad, 30 \\
  28911 Leganés, Madrid \\
  España (Spain) \\
  {\tt 100355573@alumnos.uc3m.es} \\\And
  Roxana Rodríguez Goncalves \\
  Universidad Carlos III de Madrid \\
  Avda. Universidad, 30 \\
  28911 Leganés, Madrid \\
  España (Spain) \\
  {\tt 100378389@alumnos.uc3m.es} \\\AND
  Adrián Ruiz Arroyo \\
  Universidad Carlos III de Madrid \\
  Avda. Universidad, 30 \\
  28911 Leganés, Madrid \\
  España (Spain) \\
  {\tt 100303525@alumnos.uc3m.es} \\}

\date{}

\begin{document}

\maketitle

\begin{abstract}
In this paper, we explain and discuss an architecture for word simplification based on the Mashable.com news site. This process includes content retrieval, complex word classification using machine learning, substitution with a simple counterpart and site remodeling following WCAG 2.0 guidelines. Different approaches for the complex word classifier are explored, as it is the core module for this architecture.
\end{abstract}

\section{Credits}

This document serves as a report for our final assignment at \emph{Tecnologías Emergentes en la Sociedad de la Información}, as part of the \emph{Ciencia y Tecnología Informática} master’s degree at \emph{Universidad Carlos III de Madrid}.

We would like to thank this institution and their members for their involvement in increasing knowledge and innovation.

\section{Introduction}

Cognitive impairment, aphasia and an increasing variety of diseases affect the way people reach meaning through content and can prohibit their access to mass media if obscure vocabulary or extended blocks of text are used throughout articles~\cite{feng2008text}.

In Europe, and many other developed regions, where elderly population is increasing whereas natality is decreasing~\cite{hoff2011population}, cognitive impairment will soon become a major problem, as it can also arise –normally at a milder degree– as a symptom of age~\cite{butler2004maintaining}.

There are many other groups that are also affected by complex text: second language learners and illiterates~\cite{petersen2007text}, people suffering dyslexia~\cite{rello2013simplify} and children~\cite{de2010text}, among others. In general, they need simple and common words to be able to fully understand text. Even if they could understand some complex words, the reading process would be tougher and that could discourage them from further reading~\cite{ediger2002reading}.

Barriers related to complex text can be bypassed by stablishing a series of guidelines for writers, who must adapt a simple language and thorough criteria for grammar usage~\cite{freyhoff1998make}. One of the main problems with this solution is that not every news media is willing to invest resources in accessibility. Old articles and feeds are also not affected by this solution: they will either remain as they were, or they must be rewritten to increase their accessibility.

To avoid these problems, tools for language simplification can be used. These tools work for old and new articles, and they don’t rely on news media’ approaches on accessibility or their economic resources.

For this task, one of the main objectives is improving natural language processing techniques. This article focuses on one of its topics: \textbf{complex word identification (CWI)}.

The task of complex word identification (CWI) has seen great improvement over the last few years, as machine learning capabilities and big data approaches are expanding and becoming commonplace~\cite{lohr2012age}, in contrast with the restricting requirements and economical backing they demanded in their inception~\cite{cox1997managing}.

One of the main annual events that cover this topic is the \textbf{Conference of the North American Chapter of the Association for Computacional Linguistics: Human Language Technologies (NAACL HLT)}, and their long paper submission count and acceptance rate is increasing overtime~\cite{naaclacc}.

It is then worthwhile to review literature in search for good features that can help identifying complex words, as there should be plenty of investigation already done in the area.

To that extend, the aim of this article is to \textbf{present an architecture that support the task of complex word simplification for a popular news website}. We chose Mashable.com for its entertainment and technology covering, topics that should mix both simple and complex vocabulary. After identification of complex words, we replace them with a simple alternative, so that the meaning prevails through the content. As the original site might contain elements that aren't accesible, we end the process by generating a web page derived from the original that keeps content but conforms to a well-known compendium of accessibility guidelines: \textbf{WCAG 2.0}~\cite{caldwell2008web}.

The evaluation of this architecture is centered on the classification algorithm chosen for the identification task. As of the WCAG conformance, it is checked through a validation tool. We will discuss these issues on section~\ref{sec:eval} of this paper.

\section{Previous Work}

The two critical parts of text simplification are the \textbf{identification of complex words} and the \textbf{election of a simple alternative}. These are the main topics in need of a literature review, as current solutions are still not very polished.

\subsection{Complex Word Identification (CWI)}

Some of the most recent papers that apply modern classification techniques to the CWI problem rely on a diverse array of features and the SVM algorithm. \textbf{The usage of machine learning schemes is not common in older works but has proven to offer better results than previous approaches}.

The problem with this method is that its efficacy greatly depends on the availability of a diverse and correctly classified dataset, either for training or evaluation. As each language has its own properties and peculiarities, the features that explain complex words in a classifier may only work for a single language. For common languages, like English, this is not a problem, as there are some settled datasets and dictionaries that work well for CWI. For other languages, where finding a dataset might be trickier, this is a huge problem, as \textbf{there might be no option for machine learning to be applied until a good dataset is available}.

Moreover, the classification of complex words is a consensual matter, which means there is no absolute law that distinguishes complex words from simple ones. This means there is a necessity for a significant number of human annotators –commonly, experts in linguistics– that manually classify each word of the dataset. \textbf{Arranging this board of experts requires resources, and it is a necessity if there are no available datasets or if the available datasets are not good enough}.

We chose a machine learning approach over a classical approach because we are classifying English words and there are well known datasets for this language.

In our search for relevant word features that might explain word complexity for the English language, we carried out a literature review. We explored conferences starting from year 2000 and that included a CWI section.

One of the most successful approaches, known as \textbf{SV000gg} and presented at \textbf{SemEval 2016}, is extensively explained in two articles~\cite{paetzold2016semeval} and~\cite{paetzold2016sv000gg}. We believe this is one of the most complete works available, as it relies on multiple classifiers and features to make a decision. Some of them are the usual suspects; that is, well-known algorithms that have been exhaustively tested over the years by previous authors.

The system described by the authors implements a \emph{hard voting} algorithm and a \emph{soft voting} algorithm. These algorithms decide whether a word is simple or complex by examining the output of multiple classifiers for the word and its context. The input for the classifiers contains \textbf{69 features}, which the authors categorize as:

\begin{itemize}
	\item{\textbf{Binary}: Features based on the word being part of a simple vocabulary.}
	\item{\textbf{Lexical}: Features based on the number of syllables, senses, hypernyms, word length, \dots etc.}
	\item{\textbf{Collocational}: Features related to the properties of the word's surroundings (window or n-gram).}
	\item{\textbf{Nominal}: Features related to the word's function in its context (part-of-speech or \emph{POS}).}
\end{itemize}

Moreover, the voting system applies to \textbf{21 classifiers}, that are also categorized:

\begin{itemize}
	\item{\textbf{Lexicon-based}: Algorithms that classify the word as simple or complex by consulting vocabularies.}
	\item{\textbf{Threshold-based}: Algorithms that receive features for training and define a threshold of their values to decide if future words are simple or complex.}
	\item{\textbf{Machine Learning-assisted}: Algorithms based on machine learning techniques to train a classifier. E.g.: SVM, Decision Trees, Random Forests, Stochastic Gradient Descent or Gradient Boosting and Multi-Layer Perceptrons.}
\end{itemize}

This information proved very valuable, as \emph{the hard-voting and soft-voting algorithms achived the two greatest scores in SemEval 2016}. Our approach is far less complex: \textbf{if we had implemented a system as sophisticated as SV000gg, it would have required an entire essay itself}.

Instead, we focused on the architectural aspect of web simplification. If every module is correctly defined and isolated, then the classifier can be easily improved in the future.

\subsection{Complex Word Replacement}

The task of complex word replacement is hard due to the need to \emph{keep the same meaning the complex word had in its context} and the requirement that the alternative word \emph{has to diminish the complexity of the original word}.

Complex word replacement is stil predominantly done by using sets of rules~\cite{glavavs2015simplifying}. With this rules, long and complex words are substituted by a short and simple alternative. As with complex word classification, the process of manually compiling this set of rules requires resources and can't be easily extended to different languages.

Also in~\cite{glavavs2015simplifying}, the authors describe a classifier that can report the \textbf{10 best candidates to replace a word}. The features it uses are:

\begin{itemize}
	\item{\textbf{Semantic Similarity}: Meaning comparison between one word and the other.}
	\item{\textbf{Context Similarity}: Since the meaning of a word can depend of its context, similarity between the context of one word and the usual contexts of the other.}
	\item{\textbf{Difference of Information Contents}: Meaning complexity of one word compared to the other.}
	\item{\textbf{Language Model Features}: Probability of the replacement word to appear in the context of the original word.}
\end{itemize}

In fact, these features are pretty similar to the ones used in~\cite{paetzold2016sv000gg}. The results reported by the authors, that ran the algorithm over the \textbf{SemEval 2012} dataset, seems promising. The only drawback is it only works with single words. This is not a problem however, as CWI classifiers only work with single words too.

\section{Proposed Architecture}

For text simplification of a news website, we propose a modular architecture where each module solves an individual problem. The association between each module an the problem it is designed to solve is:

\begin{itemize}
	\item{\textbf{Web Article Crawling}: For a news media web site (e.g. Mashable.com), receive the URL of one of its articles (assuming they all share a similar structure) and retrieve its title, the URL of its front picture and its main content.}
	\item{\textbf{Complex Word Identification}: For each word contained in every sentence of the news body, obtain a list of complex words and the relative position in its sentence.}
	\item{\textbf{Complex Word Replacement}: For each complex word in a sentence, obtain a suitable simple alternative and then prepare a new sentence with every complex word replaced by its alternative.}
	\item{\textbf{Accessible Web Page Generation}: For each replacement sentence, and given the title and front picture of an article, generate a web page that keeps the look-and-feel of the original web page and conforms to the WCAG 2.0~\cite{caldwell2008web} guidelines.}
\end{itemize}

To reach every possible audience and to guarantee the extensibility of the architecture, we propose two user interfaces:

\begin{itemize}
	\item{\textbf{Command Line Interface (CLI)}: This interface is operated through a terminal window in a graphical user interface, receives the URL of the article to simplify and opens a web browser window with the simplified version of the web page. We provide a reference implementation which requires a Python 3 interpreter to be installed, along with some dependencies that are listed in appendix~\ref{sec:src}}.
	\item{\textbf{Mozilla Firefox Extension}: This interface consists of a button embedded in the Mozilla Firefox web browser that, once clicked in the desired news page, loads a simplified version of the page highlighting the words that have been replaced. We provide a reference implementation of this Firefox Extension which requires some additional setup. In appendix~\ref{sec:mfx}, we provide further instruction.}
\end{itemize}

In the following subsections, we provide more insight into the \emph{complex word identification algorithm} and the \emph{complex word replacement algorithm} that we have implemented.

\subsection{Complex Word Classification Algorithm}
\label{ssec:cwia}

For word classification, a machine learning approach was chosen. Thus, we defined \textbf{10 features} based on our literature review. This features are:

\begin{itemize}
	\item{\textbf{Word length}: Complex words might be longer than simple words.}
	\item{\textbf{Number of syllables}: Complex words might feature a higher number of syllables than simple words.}
	\item{\textbf{Sentence length}: Longer sentences may have impact on the perceived complexity of the words it contains.}
	\item{\textbf{n-gram probability}: For a window of 3 elements, probability of each combination of previous and subsequent words to appear in a dataset. Unfolding this setting, a total of five features arise.}
	\item{\textbf{Number of synsets}: Complex words might have a smaller set of meanings than a simple word, as its use could be less frequent.}
	\item{\textbf{Number of hypernyms}: A complex word might be so specific that its hypernym tree is huge, while a simple word tends to stay near the top of the tree.}
\end{itemize}

The election of a well-suited machine learning algorithm is one of the most critical decisions for the classifier: complex words should not be classified as simple, otherwise they will remain present in the transformed text. This also applies the other way round: simple words should not be classified as complex, or they may be replaced by a more complex alternative. For this reason, we tested the classification accuracy of \textbf{three different classification algorithms}, in order to keep the best one:

\begin{itemize}
	\item{\textbf{SVC}: C-Support Vector Classification~\cite{chang2011libsvm}.}
	\item{\textbf{RandomForest}: Random Forest Classification.~\cite{ho1998random}.}
	\item{\textbf{GaussianNB}: Gaussian Naive Bayes Classification.~\cite{hand2001idiot}.}
\end{itemize}

Moving on to the data samples, we used a single dataset, from which we build \textbf{two different and statically-partitioned subsets}, one for \emph{training} and one for \emph{evaluation}. The only source for the data is Wikipedia, as it combines both simple and complex words. We make the dataset available to the public as well as the rest of the source code for the reference implementations (for further instructions, refer to appendix~\ref{sec:src}).

In the reference implementations, we also used the \textbf{WordNet lexical database for English}~\cite{miller1995wordnet} to obtain the synsets and hypernyms for each word.

\subsection{Complex Word Replacement Algorithm}

We took a simple approach for the task of complex word replacement. This is the main weakness in the reference implementations and we expect future works to improve this situation.

Without any evaluation of the part-of-speech (POS) –this means contextual information is ignored–, the \textbf{shortest synonym} available through WordNet is chosen.

\section{Evaluation}
\label{sec:eval}

\subsection{Classifier Performance}

To evaluate the performance of each classification algorithm listed in section~\ref{ssec:cwia}, we trained each classifier with the training partition of the dataset and then classified the samples from its evaluation partition. 

Then, having the output from the classifier, we build some statistics by comparing the expected class of each sample and the one assigned by the classifier (table~\ref{tab:eval}).

\begin{table}
	\centering
	\small
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		{\bf Algorithm} & {\bf Accu.} & {\bf Prec.} & {\bf Rec.} & {\bf F}\\\hline
		\verb|SVC| & {0.63} & {0.60} & {0.47} & {0.53} \\
		\verb|RandomForest| & {0.68} & {0.65} & {0.60} & {0.62} \\
		\verb|GaussianNB| & {0.70} & {0.65} & {0.68} & {0.66} \\\hline
	\end{tabular}
	\caption{Evaluation of the classifier output on the evaluation partition of the dataset}\label{tab:eval}
\end{table}

The results for the array of considered features show that the best classification algorithm is \textbf{GaussianNB}. Thus, we use it in our reference implementation.

However, it is no match for other state-of-the-art classification techniques, like \textbf{SV000gg}~\cite{paetzold2016sv000gg}, with a precision of \textbf{0.78}~\cite{paetzold2016semeval} over a bigger dataset.

\subsection{WCAG 2.0 Compliance}

The generated website must conform to the WCAG 2.0 guidelines~\cite{caldwell2008web}. To check whether or not the web elements and structure satisfy its accesibility criteria, we used multiple online and offline validation tools:

\begin{itemize}
	\item{\textbf{AC Checker}~\footnote{https://achecker.ca/checker/index.php}: Online validator that allows checking a public URL, uploaded file or text input.}
	\item{\textbf{A11Y Checker}~\footnote{https://github.com/Muhnad/a11y-checker}: Offline validator based on Node.js.}
	\item{\textbf{Node WCAG}~\footnote{https://github.com/cfpb/node-wcag}: Another offline validator based on Node.js but only focused on WCAG criteria.}
	\item{\textbf{WAVE Browser Extension}~\footnote{https://wave.webaim.org/extension/}: Extension for Google Chrome and Mozilla Firefox that visually highlights major issues over the webpage to evaluate.}
\end{itemize}

After a trial-and-error procedure, we ensured the WCAG 2.0 (AA) guidelines were satisfied while keeping the same look-and-feel of Mashable.com. This means the site has an acceptable degree of accessibility both in content, structure and style.

\section{Conclusions and Future Work}

In this paper, we presented a modular architecture to simplify a news website (Mashable.com). We also emphasized the value of text simplification, a field that is still under development.

Our implementation of a \emph{complex word classifier} and a \emph{complex word replacer} are simple and are no match against more complex state-of-the-art alternatives, but they allowed us to test the proposed architecture. We chose the \emph{Gaussian Naive Bayes} classifier and the \emph{shortest synonym strategy} for each of them.

The proposed architecture only allows word simplification for now, but it may be extended in the future to also support \textbf{whole-sentence simplification}. This means text from a sentence is reordered and restructured so that it is easier to read while keeping its meaning. As this issue is far more sophisticated and requires further natural language processing than complex word simplification, it will require new investigation and developments to emerge.

\bibliography{tesi_articulo}
\bibliographystyle{acl_natbib}

\appendix

\section{Source Code}
\label{sec:src}

The source code for the reference implementation is publicly available at \textbf{GitHub}~\footnote{https://github.com/gerice92/TESI} and will also be handed over with this paper.

The \verb|README.md| Markdown text file contains the dependencies, structure and execution instructions for the source code.

The \verb|doc/| directory contains the \LaTeX{} source used to build this PDF and the style files related to the NAACL HLT 2018 paper submission template. As it is an internal matter and its build is independent of every other piece of code, we do not provide instructions on how to compile it.

The \verb|src/| directory contains the Python 3 source

\section{Firefox Addon}
\label{sec:mfx}

\end{document}